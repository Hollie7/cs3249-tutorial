{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee3a0f6",
   "metadata": {},
   "source": [
    "# Backend: FastAPI + OpenAI (GPT) minimal chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28594dc0",
   "metadata": {},
   "source": [
    "1. Reads API keys & config from environment variables / .env\n",
    "2. Supports simple single-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1972571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPT Backend running at http://localhost:8001/chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [61758]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import os, uvicorn, threading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if present\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat_endpoint(req: ChatRequest):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": req.message}]\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "    return {\"response\": reply}\n",
    "\n",
    "def run_backend():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n",
    "\n",
    "thread = threading.Thread(target=run_backend, daemon=True)\n",
    "thread.start()\n",
    "print(\"✅ GPT Backend running at http://localhost:8001/chat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e3df7",
   "metadata": {},
   "source": [
    "# Extended GPT backend (Multi-turn convesation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc5ded",
   "metadata": {},
   "source": [
    "1. Reads API keys & config from environment variables / .env\n",
    "2. Supports simple **multi-turn conversation**\n",
    "3. Supports setting of **temperature & max_tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6da284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import os, uvicorn, threading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    temperature: float | None = 0.7 # [NEW]\n",
    "    max_tokens: int | None = 200 # [NEW]\n",
    "\n",
    "# [NEW] global history for a single conversation\n",
    "HISTORY: list[dict[str, str]] = []\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat_endpoint(req: ChatRequest):\n",
    "    # append user message\n",
    "    HISTORY.append({\"role\": \"user\", \"content\": req.message})\n",
    "\n",
    "    print(req.temperature)\n",
    "    print(req.max_tokens)\n",
    "\n",
    "    # call GPT with accumulated history\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=HISTORY,\n",
    "        temperature=float(req.temperature) if hasattr(req, \"temperature\") else 0.7, # [NEW]\n",
    "        max_tokens=int(req.max_tokens) if hasattr(req, \"max_tokens\") else 200 # [NEW]\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "\n",
    "    # append assistant reply\n",
    "    HISTORY.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    return {\"response\": reply}\n",
    "\n",
    "def run_backend():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n",
    "\n",
    "# Avoid duplicate threads in Jupyter\n",
    "try:\n",
    "    thread\n",
    "    already_running = thread.is_alive()\n",
    "except NameError:\n",
    "    already_running = False\n",
    "\n",
    "if not already_running:\n",
    "    thread = threading.Thread(target=run_backend, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "print(\"✅ Minimal Multi-Turn GPT Backend running at http://localhost:8001/chat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fb6a3",
   "metadata": {},
   "source": [
    "# Extended GPT backend (Multi-turn convesation + Sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cf9c3",
   "metadata": {},
   "source": [
    "1. Reads API keys & config from environment variables / .env\n",
    "2. Provides a single /chat endpoint\n",
    "3. Supports simple **multi-turn context** via an in-memory session store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import os, uvicorn, threading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if present\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    session_id: str | None = None   # [NEW] allow session_id from frontend\n",
    "\n",
    "# [NEW] in-memory session store\n",
    "SESSIONS: dict[str, list[dict[str, str]]] = {}\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat_endpoint(req: ChatRequest):\n",
    "    # [NEW] resolve session_id\n",
    "    sid = req.session_id or \"default\"\n",
    "    if sid not in SESSIONS:\n",
    "        SESSIONS[sid] = []\n",
    "\n",
    "    # [NEW] append new user message to session history\n",
    "    history = SESSIONS[sid]\n",
    "    history.append({\"role\": \"user\", \"content\": req.message})\n",
    "\n",
    "    # [CHANGED] use history instead of single-turn\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=history\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "\n",
    "    # [NEW] append assistant reply to session history\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    return {\"response\": reply, \"session_id\": sid}  # [CHANGED] return session_id too\n",
    "\n",
    "def run_backend():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001)\n",
    "\n",
    "# Avoid re-running multiple uvicorn servers in Jupyter\n",
    "try:\n",
    "    thread\n",
    "    already_running = thread.is_alive()\n",
    "except NameError:\n",
    "    already_running = False\n",
    "\n",
    "if not already_running:\n",
    "    thread = threading.Thread(target=run_backend, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "print(\"✅ GPT Backend with Multi-Turn running at http://localhost:8001/chat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b579b",
   "metadata": {},
   "source": [
    "# Extended GPT backend (Multi-turn convesation + CORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d1ad8",
   "metadata": {},
   "source": [
    "1. Reads API keys & config from environment variables / .env\n",
    "2. Provides a single /chat endpoint\n",
    "3. Supports simple multi-turn context via an in-memory session store\n",
    "4. **Safe error handling and CORS for local dev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from typing import Dict, List\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load .env if present\n",
    "\n",
    "# --- OpenAI new SDK (>= 1.0) ---\n",
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is missing. Put it in your environment or a .env file.\")\n",
    "\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=OPENAI_BASE_URL\n",
    ")\n",
    "\n",
    "# ------------- FastAPI app -------------\n",
    "app = FastAPI(title=\"CS3249 GPT Backend\")\n",
    "\n",
    "# Allow local frontends to call this API\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],    # for teaching/demo; restrict in production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Simple in-memory session store: { session_id: [{\"role\": \"...\", \"content\": \"...\"}] }\n",
    "SESSIONS: Dict[str, List[Dict[str, str]]] = {}\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    session_id: str | None = None  # optional: supply a session id to keep context\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    session_id: str\n",
    "\n",
    "def get_or_create_session(session_id: str | None) -> str:\n",
    "    \"\"\"Return a session_id and ensure it exists in memory.\"\"\"\n",
    "    if not session_id:\n",
    "        # naive unique id; good enough for class demo\n",
    "        session_id = f\"sess-{len(SESSIONS) + 1}\"\n",
    "    if session_id not in SESSIONS:\n",
    "        SESSIONS[session_id] = []\n",
    "    return session_id\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "def chat_endpoint(req: ChatRequest):\n",
    "    try:\n",
    "        # 1) resolve session + append the user message\n",
    "        sid = get_or_create_session(req.session_id)\n",
    "        history = SESSIONS[sid]\n",
    "\n",
    "        # Build messages for Chat Completions API\n",
    "        # Keep it minimal: system prompt optional; add if you want a consistent persona\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful, concise teaching assistant.\"}]\n",
    "        messages.extend(history)\n",
    "        messages.append({\"role\": \"user\", \"content\": req.message})\n",
    "\n",
    "        # 2) call OpenAI Chat Completions\n",
    "        completion = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        reply = completion.choices[0].message.content\n",
    "\n",
    "        # 3) update session with assistant reply\n",
    "        history.append({\"role\": \"user\", \"content\": req.message})\n",
    "        history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "        return ChatResponse(response=reply, session_id=sid)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Surface error to the caller but keep JSON shape stable\n",
    "        return ChatResponse(response=f\"[Backend error: {e}]\", session_id=req.session_id or \"unknown\")\n",
    "\n",
    "def run_backend():\n",
    "    # Use a non-8000 port to avoid clashes with other demos\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8001, log_level=\"info\")\n",
    "\n",
    "# Start once per Notebook execution\n",
    "try:\n",
    "    thread\n",
    "    already_running = thread.is_alive()\n",
    "except NameError:\n",
    "    already_running = False\n",
    "\n",
    "if not already_running:\n",
    "    thread = threading.Thread(target=run_backend, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "print(\"✅ GPT Backend running at http://localhost:8001/chat\")\n",
    "print(f\"   Model: {OPENAI_MODEL} | Base URL: {OPENAI_BASE_URL}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
